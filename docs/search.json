[
  {
    "objectID": "posts/benefits_of_bayesian_beta/index.html",
    "href": "posts/benefits_of_bayesian_beta/index.html",
    "title": "What Makes Bayesian Methods Different and Why Should I Care?",
    "section": "",
    "text": "In my last post, I showed how to estimate risk of sudden unexpected infant death (SUID) in census tracts using Bayesian methods. The Bayesian approach is a separate branch of statistics from the Frequentist approach that most of us learned in school. Historically, Frequentist methods were so dominantly favored, many people may not have been aware that the statistics they were learning were termed “Frequentist” and that there was an alternative “Bayesian” approach available.\nMy objective for this post is to outline enough of the Bayesian approach to explain why it is better suited to the use-case of estimating SUID risk in small areas, then to give a set of examples to build intuition around how the method is working under the hood.\nFor a more in-depth introduction to Bayesian statistics, check out the textbook “Bayes Rules! An Introduction to Applied Bayesian Modeling”, which is available online for free. I adapted the source code from it’s companion package {bayesrules} to develop the visualizations in this post."
  },
  {
    "objectID": "posts/benefits_of_bayesian_beta/index.html#the-challenge",
    "href": "posts/benefits_of_bayesian_beta/index.html#the-challenge",
    "title": "What Makes Bayesian Methods Different and Why Should I Care?",
    "section": "The Challenge",
    "text": "The Challenge\nEpidemiology works with seemingly idiosyncratic denominators when representing incidence or risk. For example, the overall incidence of SUID in Cook County, IL for 2014 was 88.3 cases per 100,000 live births. While it would be just as mathematically valid to represent it as 0.000883 cases per single live birth, that doesn’t make sense in human terms. When considering a rare event, you want to use a large pool of people as your reference denominator.\nLet’s assume the incidence of 88.3 accurately represents the underlying risk of SUID in the county for 2014. If we know that there were 140,000 live births in the next five years, then we would expect there to be about 124 cases of SUID.\n\\[Expected \\space New \\space Cases \\space in \\space the \\space County = Incidence \\times Live \\space Births =\\]\n\\[\\frac{88.3 \\space cases}{100,000 \\space live \\space births} \\times 140,000 \\space live \\space births \\approx 124 \\space cases \\]\nBut let’s zoom in to a single census tract where there were 240 live births over the same time period. Using the formula above, we would expect to observe zero cases. However, given what we know about incidence for the whole county, we wouldn’t observe zero cases and assume the underlying risk for that individual tract was actually zero. Estimating risk is hard when an event is rare and/or the reference pool of people is small."
  },
  {
    "objectID": "posts/benefits_of_bayesian_beta/index.html#why-use-bayesian-methods-to-solve-this-challenge",
    "href": "posts/benefits_of_bayesian_beta/index.html#why-use-bayesian-methods-to-solve-this-challenge",
    "title": "What Makes Bayesian Methods Different and Why Should I Care?",
    "section": "Why Use Bayesian Methods to Solve this Challenge?",
    "text": "Why Use Bayesian Methods to Solve this Challenge?\nThe “Frequentist” approach to statistics uses data in isolation from “prior knowledge”. If we observe 0 cases out of 240 live births in a census tract, our estimate of the underlying risk is 0 cases per 100,000 live births–even if our knowledge about the county as a whole suggests the underlying risk is closer to 88.3. Furthermore, if we want to make inferences about our data, Frequentist methods require a certain threshold sample size (for example n = 30) to be accurate. If the pool of live births in a census tract is something small like 13 total, than we expect up front that our estimate is going to be out of whack.\nIn contrast, Bayesian methods weigh the observed data against prior knowledge. If we have a lot of observed data, the final estimate will rely mostly on that data and look similar to the Frequentist estimate. But if we don’t observe a lot of data, the prior knowledge weighs more heavily into the estimate and will diverge away from the Frequentist estimate."
  },
  {
    "objectID": "posts/benefits_of_bayesian_beta/index.html#examples",
    "href": "posts/benefits_of_bayesian_beta/index.html#examples",
    "title": "What Makes Bayesian Methods Different and Why Should I Care?",
    "section": "Examples",
    "text": "Examples\nAnother important distinguishing characteristic of Bayesian methods, is that they represent the uncertainty of an estimate using a distribution. In my previous post, we used a “Beta distribution” and outlined our process for specifying its parameters to represent our prior knowledge as follows:\n\n\nCode\nlibrary(tidyverse)\n\nshape1 <- 0.189\nshape2 <- 213\n\nggplot(tibble(x = c(0, 1)), aes(x)) +\n    stat_function(\n        fun = dbeta,\n        args = list(\n            shape1 = shape1,\n            shape2 = shape2\n        ),\n        color = \"red\"\n    ) +\n    stat_function(\n        fun = dbeta,\n        args = list(\n            shape1 = shape1,\n            shape2 = shape2\n        ),\n        geom = \"area\",\n        fill = \"pink\",\n        alpha = 0.5,\n    ) +\n    labs(\n        \n        x = \"SUID Risk (cases per 100,000 live births)\",\n        y = \"Density\"\n    ) +\n    scale_y_sqrt() +\n    scale_x_sqrt(labels = scales::label_comma(scale = 1E5)) +\n    coord_cartesian(xlim = c(0, 0.1)) +\n    theme_light()\n\n\n\n\n\nThe shaded red area represents the full range of plausible SUID risk values represented by our prior knowledge. The point where the curve is highest corresponds to the most plausible value (0) and we can average over the whole curve to get an expected risk of 88.3. These are useful point estimates but we hold onto the whole distribution to say the true risk could vary anywhere from 0 to 2500. Next, we can overlay a range of plausible values based on hypothetical data where we observed 1 case of SUID in 270 live births:\n\n\nCode\ny <- 1\nn <- 270\n\nlike_scaled <- function(x) {\n    like_fun <- function(x) {\n        dbinom(x = y, size = n, prob = x)\n    }\n    scale_c <- integrate(like_fun, lower = 0, upper = 1)[[1]]\n    like_fun(x)/scale_c\n}\n\nggplot(tibble(x = c(0, 1)), aes(x)) +\n    stat_function(\n        fun = dbeta,\n        args = list(\n            shape1 = shape1,\n            shape2 = shape2\n        ),\n        color = \"red\"\n    ) +\n    stat_function(\n        fun = dbeta,\n        args = list(\n            shape1 = shape1,\n            shape2 = shape2\n        ),\n        geom = \"area\",\n        fill = \"pink\",\n        alpha = 0.5,\n    ) +\n    stat_function(\n        fun = like_scaled, \n        color = \"blue\"\n    ) +\n    stat_function(\n        fun = like_scaled,\n        geom = \"area\",\n        fill = \"lightblue\",\n        alpha = 0.5\n    ) +\n    labs(\n        title = paste0(\"Cases = \", as.character(y), \", Live Births = \", as.character(n)),\n        x = \"SUID Risk (cases per 100,000 live births)\",\n        y = \"Density\"\n    ) +\n    scale_y_sqrt() +\n    scale_x_sqrt(labels = scales::label_comma(scale = 1E5)) +\n    coord_cartesian(xlim = c(0, 0.1)) +\n    theme_light()\n\n\n\n\n\nThe data alone thinks the most plausible risk values are much higher than our prior understanding suggested. Bayesian methods help us weigh between these two somewhat conflicting sources of information by combining the prior expectations and the observed data to give us a “posterior” estimate:\n\n\nCode\nplot_beta_binom_variation <- function(shape1_num, shape2_num, y_num, n_num) {\n    \n    like_scaled <- function(x) {\n        like_fun <- function(x) {\n            dbinom(x = y_num, size = n_num, prob = x)\n         }\n        scale_c <- integrate(like_fun, lower = 0, upper = 1)[[1]]\n        like_fun(x)/scale_c\n    }\n\n    prior_fun <- function(x) {\n        dbeta(x, shape1 = shape1_num, shape2 = shape2_num)\n    }\n\n    posterior_fun <- function(x) {\n        dbeta(x, shape1 = shape1_num + y_num, shape2 = shape2_num + n_num - y_num)\n    }\n    \n    ggplot(tibble(x = c(0, 1)), aes(x)) +\n    stat_function(fun = prior_fun, aes(color = \"Prior\"), alpha = 0.75) +\n    stat_function(fun = like_scaled, aes(color = \"Data\"), alpha = 0.75) +\n    stat_function(fun = posterior_fun, aes(color = \"Posterior\"), alpha = 0.75) +\n    geom_vline(\n        xintercept = (shape1_num + y_num) / (shape1_num + shape2_num + n_num - y_num),\n        linetype = \"dashed\"\n    ) +\n    labs(\n        title = paste0(\"Cases = \", as.character(y_num), \", Live Births = \", as.character(n_num), \",\\n Average Posterior Estimate = \", as.character(round((shape1_num + y_num) / (shape1_num + shape2_num + n_num - y_num) * 1E5)), \" cases per 100,000 live births\"),\n        x = \"SUID Risk (cases per 100,000 live births)\",\n        y = \"Density\"\n    ) +\n    scale_y_sqrt() +\n    scale_x_sqrt(labels = scales::label_comma(scale = 1E5)) +\n    coord_cartesian(xlim = c(0, 0.1)) +\n    scale_color_manual(\n        \"\",\n        values = c(\n            Prior = \"red\",\n            `Data` = \"blue\",\n            Posterior = \"purple\"\n        ),\n        breaks = c(\n            \"Prior\",\n            \"Data\",\n            \"Posterior\"\n        )\n    ) +\n    theme_light()\n}\n\nplot_beta_binom_variation(0.189, 213, 1, 270)\n\n\n\n\n\nI’ve taken out the shaded regions for visual clarity, but the area under each curve is still representing the most plausible values. Look how the purple posterior curve distributes itself in between the red and blue ones. Bayesian statistics weighed our prior expectations against the data and estimated the true risk to be somewhere in between.\nTo get some better intuition about what’s happening, let’s look at some extreme scenarios. First, what if we observed zero cases in a census tract with very few live births?\n\n\nCode\nplot_beta_binom_variation(0.189, 213, 0, 10)\n\n\n\n\n\nIn this example, the data gave us very little information so the prior expectation and posterior estimation curves look almost identical. The blue curve is very spread out, indicating a wide range of plausible risk values based on the data alone.\n\n\nCode\nplot_beta_binom_variation(0.189, 213, 0, 1000)\n\n\n\n\n\nThis example considers a census tract where we still observed zero cases, but there were many live births, meaning there were more opportunities for a case of SUID to happen. Given there were so many observed live births, the data weighed much more heavily into the posterior, which estimated the true risk (16) to be lower than our prior expectations (88.3).\n\n\nCode\nplot_beta_binom_variation(0.189, 213, 5, 1000)\n\n\n\n\n\nIn this example, we still observed 1000 live births, so the data heavily weighed into our posterior, but we increased the case count to 5, so the estimated true risk increased to a whopping 429.\n\n\nCode\nplot_beta_binom_variation(0.189, 213, 5, 50)\n\n\n\n\n\nAnd finally, in this example, we still observed 5 cases, but we decreased the number of live births to 50. Using the data alone, we calculated the risk to be very high, but the posterior curve hung back toward the prior curve because the weight of our observed data was less than when we observed 1000 live births."
  },
  {
    "objectID": "posts/benefits_of_bayesian_beta/index.html#wrap-up",
    "href": "posts/benefits_of_bayesian_beta/index.html#wrap-up",
    "title": "What Makes Bayesian Methods Different and Why Should I Care?",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nSome people argue that Bayesian statistics introduce too much subjectivity by letting us incorporate “arbitrary” prior expectations. Bayesians counter that statistical analysis always involves subjective interpretation. When we look at risk estimates based on the data alone, we adjust our trust in those numbers based on external knowledge. Bayesian methods simply give us a way to more formally define and weigh those expectations into our estimates. In the case of SUID risk for small populations in small geographic areas, it’s really helpful to moderate extreme estimates created by limited data using the Bayesian prior."
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "",
    "text": "Data was sourced from the Cook County Medical Examiner’s Office and modified according to the data pipeline in this commit of my Github repository.\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggdist)\n\ntheme_set(theme_light())\n\n# Parquet file generated from the ccme_archive_generic target in data pipeline linked above\nccme <- arrow::read_parquet(here::here(\"data\", \"ccme_archive_generic_2022_10_02.parquet\"))\n\ngsw_deaths <-\n    ccme |> \n    filter(gun_related == TRUE) |> \n    filter(death_date > ymd(\"2014-12-31\") & death_date < ymd(\"2022-01-01\")) |> \n    mutate(\n        manner = factor(manner, levels = c(NA, \"SUICIDE\", \"HOMICIDE\", \"ACCIDENT\"), ordered = TRUE),\n        race = factor(race, levels = c(\"Other\", \"White\", \"Black\", \"Asian\", \"Am. Indian\", NA), ordered = TRUE)\n    )\n\n\n\n\n\nImage Source: https://www.chicagotribune.com/news/ct-viz-cta-red-line-station-shooting-20200229-uaofbvsknbfv3g4xlqph4kdudi-photogallery.html"
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html#age-unstratified",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html#age-unstratified",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "Age Unstratified",
    "text": "Age Unstratified\n\nDensityDeciles\n\n\n\n\nCode\ngsw_deaths |> \n    ggplot(aes(x = age)) +\n    stat_dotsinterval() +\n    labs(\n        x = \"Age\",\n        y = \"Density\",\n        caption = \"Point is 50%ile; Thick interval contains inner 66% of data; Thin interval contains inner 95%\"\n    )\n\n\n\n\n\n\n\n\n\nCode\nquantile(gsw_deaths$age, probs = seq(0, 1, 0.1), na.rm = TRUE)\n\n\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n   0   18   21   23   25   28   31   36   42   55   95"
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-manner-of-death",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-manner-of-death",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "Age Stratified by Manner of Death",
    "text": "Age Stratified by Manner of Death\n\nDensityDeciles\n\n\n\n\nCode\ngsw_deaths |> \n    ggplot(aes(x = age, y = manner)) +\n    stat_dotsinterval() +\n    labs(\n        x = \"Age\", y = \"\",\n        caption = \"Point is 50%ile; Thick interval contains inner 66% of data; Thin interval contains inner 95%\"\n    )\n\n\n\n\n\n\n\n\n\nCode\ngsw_deaths |> \n    group_by(manner) |> \n    summarise(\n        \"0th Percentile\" = round(quantile(age, probs = 0, na.rm = TRUE)),\n        \"10th Percentile\" = round(quantile(age, probs = 0.1, na.rm = TRUE)),\n        \"20th Percentile\" = round(quantile(age, probs = 0.2, na.rm = TRUE)),\n        \"30th Percentile\" = round(quantile(age, probs = 0.3, na.rm = TRUE)),\n        \"40th Percentile\" = round(quantile(age, probs = 0.4, na.rm = TRUE)),\n        \"50th Percentile\" = round(quantile(age, probs = 0.5, na.rm = TRUE)),\n        \"60th Percentile\" = round(quantile(age, probs = 0.6, na.rm = TRUE)),\n        \"70th Percentile\" = round(quantile(age, probs = 0.7, na.rm = TRUE)),\n        \"80th Percentile\" = round(quantile(age, probs = 0.8, na.rm = TRUE)),\n        \"90th Percentile\" = round(quantile(age, probs = 0.9, na.rm = TRUE)),\n        \"100th Percentile\" = round(quantile(age, probs = 1, na.rm = TRUE)),\n    ) |> \n    pivot_longer(cols = 2:12, names_to = \" \", values_to = \"value\") |> \n    pivot_wider(names_from = manner, values_from = value) |> \n    select(\n        \" \",\n        \"NA, n=39\" = \"NA\",\n        \"Accident, n=10\" = ACCIDENT,\n        \"Homicide, n=5156\" = HOMICIDE,\n        \"Suicide, n=1064\" = SUICIDE,\n    ) |> \n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNA, n=39\nAccident, n=10\nHomicide, n=5156\nSuicide, n=1064\n\n\n\n\n0th Percentile\n2\n3\n0\n11\n\n\n10th Percentile\n7\n15\n18\n22\n\n\n20th Percentile\n17\n20\n20\n27\n\n\n30th Percentile\n18\n22\n22\n33\n\n\n40th Percentile\n21\n27\n24\n41\n\n\n50th Percentile\n24\n32\n27\n48\n\n\n60th Percentile\n26\n34\n29\n55\n\n\n70th Percentile\n40\n40\n32\n61\n\n\n80th Percentile\n47\n51\n37\n68\n\n\n90th Percentile\n59\n55\n45\n78\n\n\n100th Percentile\n63\n70\n88\n95"
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-gender",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-gender",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "Age Stratified by Gender",
    "text": "Age Stratified by Gender\n\nDensityDeciles\n\n\n\n\nCode\ngsw_deaths |> \n    ggplot(aes(x = age, y = gender)) +\n    stat_dotsinterval() +\n    labs(\n        x = \"Age\", y = \"\",\n        caption = \"Point is 50%ile; Thick interval contains inner 66% of data; Thin interval contains inner 95%\"\n    )\n\n\n\n\n\n\n\n\n\nCode\ngsw_deaths |> \n    group_by(gender) |> \n    summarise(\n        \"0th Percentile\" = round(quantile(age, probs = 0, na.rm = TRUE)),\n        \"10th Percentile\" = round(quantile(age, probs = 0.1, na.rm = TRUE)),\n        \"20th Percentile\" = round(quantile(age, probs = 0.2, na.rm = TRUE)),\n        \"30th Percentile\" = round(quantile(age, probs = 0.3, na.rm = TRUE)),\n        \"40th Percentile\" = round(quantile(age, probs = 0.4, na.rm = TRUE)),\n        \"50th Percentile\" = round(quantile(age, probs = 0.5, na.rm = TRUE)),\n        \"60th Percentile\" = round(quantile(age, probs = 0.6, na.rm = TRUE)),\n        \"70th Percentile\" = round(quantile(age, probs = 0.7, na.rm = TRUE)),\n        \"80th Percentile\" = round(quantile(age, probs = 0.8, na.rm = TRUE)),\n        \"90th Percentile\" = round(quantile(age, probs = 0.9, na.rm = TRUE)),\n        \"100th Percentile\" = round(quantile(age, probs = 1, na.rm = TRUE)),\n    ) |> \n    pivot_longer(cols = 2:12, names_to = \" \", values_to = \"value\") |> \n    pivot_wider(names_from = gender, values_from = value) |> \n    select(\n        \" \",\n        \"Female, n=557\" = \"Female\",\n        \"Male, n=5712\" = \"Male\",\n    ) |> \n    knitr::kable()\n\n\n\n\n\n\nFemale, n=557\nMale, n=5712\n\n\n\n\n0th Percentile\n0\n0\n\n\n10th Percentile\n18\n18\n\n\n20th Percentile\n20\n21\n\n\n30th Percentile\n23\n23\n\n\n40th Percentile\n26\n25\n\n\n50th Percentile\n28\n28\n\n\n60th Percentile\n32\n31\n\n\n70th Percentile\n38\n35\n\n\n80th Percentile\n46\n42\n\n\n90th Percentile\n56\n55\n\n\n100th Percentile\n88\n95"
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-race",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-race",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "Age Stratified by Race",
    "text": "Age Stratified by Race\n\nDensityDeciles\n\n\n\n\nCode\ngsw_deaths |> \n    ggplot(aes(x = age, y = race)) +\n    stat_dotsinterval() +\n    labs(\n        x = \"Age\", y = \"\",\n        caption = \"Point is 50%ile; Thick interval contains inner 66% of data; Thin interval contains inner 95%\"\n    )\n\n\n\n\n\n\n\n\n\nCode\ngsw_deaths |> \n    group_by(race) |> \n    summarise(\n        \"0th Percentile\" = round(quantile(age, probs = 0, na.rm = TRUE)),\n        \"10th Percentile\" = round(quantile(age, probs = 0.1, na.rm = TRUE)),\n        \"20th Percentile\" = round(quantile(age, probs = 0.2, na.rm = TRUE)),\n        \"30th Percentile\" = round(quantile(age, probs = 0.3, na.rm = TRUE)),\n        \"40th Percentile\" = round(quantile(age, probs = 0.4, na.rm = TRUE)),\n        \"50th Percentile\" = round(quantile(age, probs = 0.5, na.rm = TRUE)),\n        \"60th Percentile\" = round(quantile(age, probs = 0.6, na.rm = TRUE)),\n        \"70th Percentile\" = round(quantile(age, probs = 0.7, na.rm = TRUE)),\n        \"80th Percentile\" = round(quantile(age, probs = 0.8, na.rm = TRUE)),\n        \"90th Percentile\" = round(quantile(age, probs = 0.9, na.rm = TRUE)),\n        \"100th Percentile\" = round(quantile(age, probs = 1, na.rm = TRUE)),\n    ) |> \n    pivot_longer(cols = 2:12, names_to = \" \", values_to = \"value\") |> \n    pivot_wider(names_from = race, values_from = value) |> \n    select(\n        \" \",\n        \"NA, n=7\" = \"NA\",\n        \"Am. Indian, n=4\" = \"Am. Indian\",\n        \"Asian, n=33\" = Asian,\n        \"Black, n=4438\" = Black,\n        \"White, n=1742\" = White,\n        \"Other, n=45\" = Other\n    ) |> \n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNA, n=7\nAm. Indian, n=4\nAsian, n=33\nBlack, n=4438\nWhite, n=1742\nOther, n=45\n\n\n\n\n0th Percentile\n20\n20\n16\n0\n2\n14\n\n\n10th Percentile\n27\n26\n25\n18\n19\n19\n\n\n20th Percentile\n34\n33\n28\n20\n22\n21\n\n\n30th Percentile\n36\n39\n32\n23\n25\n21\n\n\n40th Percentile\n39\n41\n35\n25\n29\n24\n\n\n50th Percentile\n42\n41\n40\n27\n34\n27\n\n\n60th Percentile\n44\n41\n45\n29\n42\n29\n\n\n70th Percentile\n46\n42\n50\n32\n50\n33\n\n\n80th Percentile\n48\n46\n58\n37\n59\n37\n\n\n90th Percentile\n59\n50\n67\n45\n71\n43\n\n\n100th Percentile\n70\n54\n73\n94\n95\n66"
  },
  {
    "objectID": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-latino-ethnicity",
    "href": "posts/gun_related_deaths_from_ccmeo/index.html#age-stratified-by-latino-ethnicity",
    "title": "Characterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021",
    "section": "Age Stratified by Latino Ethnicity",
    "text": "Age Stratified by Latino Ethnicity\n\nDensityDeciles\n\n\n\n\nCode\ngsw_deaths |> \n    ggplot(aes(x = age, y = latino)) +\n    stat_dotsinterval() +\n    labs(\n        x = \"Age\", y = \"\",\n        caption = \"Point is 50%ile; Thick interval contains inner 66% of data; Thin interval contains inner 95%\"\n    )\n\n\n\n\n\n\n\n\n\nCode\ngsw_deaths |> \n    group_by(latino) |> \n    summarise(\n        \"0th Percentile\" = round(quantile(age, probs = 0, na.rm = TRUE)),\n        \"10th Percentile\" = round(quantile(age, probs = 0.1, na.rm = TRUE)),\n        \"20th Percentile\" = round(quantile(age, probs = 0.2, na.rm = TRUE)),\n        \"30th Percentile\" = round(quantile(age, probs = 0.3, na.rm = TRUE)),\n        \"40th Percentile\" = round(quantile(age, probs = 0.4, na.rm = TRUE)),\n        \"50th Percentile\" = round(quantile(age, probs = 0.5, na.rm = TRUE)),\n        \"60th Percentile\" = round(quantile(age, probs = 0.6, na.rm = TRUE)),\n        \"70th Percentile\" = round(quantile(age, probs = 0.7, na.rm = TRUE)),\n        \"80th Percentile\" = round(quantile(age, probs = 0.8, na.rm = TRUE)),\n        \"90th Percentile\" = round(quantile(age, probs = 0.9, na.rm = TRUE)),\n        \"100th Percentile\" = round(quantile(age, probs = 1, na.rm = TRUE)),\n    ) |> \n    pivot_longer(cols = 2:12, names_to = \" \", values_to = \"value\") |> \n    pivot_wider(names_from = latino, values_from = value) |> \n    select(\n        \" \",\n        \"True, n=897\" = \"TRUE\",\n        \"False, n=5372\" = \"FALSE\",\n    ) |> \n    knitr::kable()\n\n\n\n\n\n\nTrue, n=897\nFalse, n=5372\n\n\n\n\n0th Percentile\n2\n0\n\n\n10th Percentile\n18\n18\n\n\n20th Percentile\n20\n21\n\n\n30th Percentile\n21\n23\n\n\n40th Percentile\n23\n26\n\n\n50th Percentile\n26\n28\n\n\n60th Percentile\n28\n32\n\n\n70th Percentile\n32\n36\n\n\n80th Percentile\n39\n43\n\n\n90th Percentile\n47\n57\n\n\n100th Percentile\n84\n95"
  },
  {
    "objectID": "posts/model_live_births_from_pop_count/index.html",
    "href": "posts/model_live_births_from_pop_count/index.html",
    "title": "Modeling Live Births from Population Counts: A First-Time MCMC Model",
    "section": "",
    "text": "In my previous post, I estimated risk of sudden unexpected infant death in census tracts while acknowledging that I was using the wrong denominator. I should have been using the count of live births in each census tract, but I have not been able to find counts of live births at such a fine geographic scale. The data we do have available from the U.S. Census is the count of children under five years old (5yo) per census tract, so I was using that as a proxy for the count of live births over a five year period. In the future, I want to be more accurately approximate the true denominator.\nMy objective for this blog post is to fit models of counts of live births regressed from counts of children under 5yo at the county level so I can estimate counts of live births at the census tract level.\nSidenote: Despite my enthusiasm for Bayesian statistics, I have little practice with fitting models using MCMC methods, so this is also a chance for me to explore doing so with the R package {brms}. Section 6.2 of “Bayes Rules” describes MCMC methods more in detail if interested, but if not, just note that when Bayesian models get sufficiently complex, you use MCMC sampling to approximate a model solution when it would otherwise be too mathematically difficult.\n\n\n\nDon’t worry, I’m not sure I do either."
  },
  {
    "objectID": "posts/model_live_births_from_pop_count/index.html#data-exploration",
    "href": "posts/model_live_births_from_pop_count/index.html#data-exploration",
    "title": "Modeling Live Births from Population Counts: A First-Time MCMC Model",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nlibrary(tidyverse)\nlibrary(ggdist)\nlibrary(brms)\nlibrary(insight)\nlibrary(performance)\n\ntheme_set(theme_light())\n\nmodel_data <- read_csv(here::here(\"data\", \"births_and_under_five_by_county_2022_09_17.csv\"))\ntract_data <- read_csv(here::here(\"data\", \"under_five_by_tract_2022_09_19.csv\"))\n\nLet’s start with some exploration of the variables and their relationships in the model data: Here’s a glimpse at what we’re working with:\n\n\nCode\nglimpse(model_data)\n\n\nRows: 3,142\nColumns: 3\n$ GEOID          <chr> \"01099\", \"01079\", \"01077\", \"01097\", \"01009\", \"01067\", \"…\n$ pop_under_five <dbl> 20733, 32924, 92729, 413210, 57826, 17205, 164542, 2970…\n$ births         <dbl> 1052, 1804, 4652, 27790, 3318, 837, 9488, 1513, 6555, 1…\n\n\n\nUnivariate\nHere is the distribution of our county-level target parameter, counts of live births:\n\n\nCode\nmodel_data |> \n    ggplot(aes(x = births)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Live Births in Each County of the USA from 2015-2019\",\n        x = \"Count of Live Births\",\n        y = \"Density\"\n    )\n\n\n\n\n\nThis distribution is highly skewed right, so it’s hard to intuit visually. As a couple of supplements, here’s a table of percentiles:\n\n\nCode\nquantile(model_data$births)\n\n\n    0%    25%    50%    75%   100% \n     0    612   1481   3931 623037 \n\n\nAnd here is the same distribution on a log-transformed x-axis:\n\n\nCode\nmodel_data |> \n    ggplot(aes(x = births + 1E-10)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Live Births in Each County of the USA from 2015-2019\",\n        x = \"Count of Live Births on Log Scale\",\n        y = \"Density\"\n    ) +\n    scale_x_log10()\n\n\n\n\n\nNotice the one county with an outlier of purportedly zero births.\nPerhaps unsurprisingly, we observe similar trends from our predictor variable, the count of children under 5yo (pop_under_five):\n\n\nCode\nggplot(model_data, aes(x = pop_under_five)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Children Under 5yo in Each County of the USA from 2015-2019\",\n        x = \"Count of Children Under 5yo\",\n        y = \"Density\"\n    )\n\n\n\n\n\n\n\nCode\nquantile(model_data$pop_under_five)\n\n\n         0%         25%         50%         75%        100% \n      86.00    10902.50    25726.00    68072.75 10039107.00 \n\n\n\n\nCode\nggplot(model_data, aes(x = pop_under_five)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Children Under 5yo in Each County of the USA from 2015-2019\",\n        x = \"Count of Children Under 5yo on Log Scale\",\n        y = \"Density\"\n    ) +\n    scale_x_log10()\n\n\n\n\n\n\n\nBivariate\nLet’s test whether our target variable and predictor have a linear relationship:\n\n\nCode\nmodel_data |> \n    correlation::cor_test(x = \"pop_under_five\", y = \"births\") |> \n    plot() +\n    labs(\n        x = \"Count of Children Under 5yo\",\n        y = \"Count of Live Births\"\n    )\n\n\n\n\n\nAwesome, the correlation coefficient is extremely high and the p value is below 0.01, let’s make a linear model! Not so fast, as noted earlier, it’s hard to assess these relationships visually, because they are so skewed. Indeed, if you log transform either axis, you start to see how the relationship breaks down:\n\n\nCode\nmodel_data |> \n    correlation::cor_test(x = \"pop_under_five\", y = \"births\") |> \n    plot() +\n    labs(\n        x = \"Count of Children Under 5yo on a Log Scale\",\n        y = \"Count of Live Births\"\n    ) +\n    scale_x_log10()\n\n\n\n\n\nIt looks like a linear line describes the relationship well in the middle range, but not so much at small or large values of the predictor variable. To get a better understanding, let’s go ahead and fit a linear model and check its assumptions:\n\n\nCode\nfreq_linear_model <- lm(births ~ pop_under_five, data = model_data)\n\ncheck_model(\n    freq_linear_model, \n    check = c(\"qq\", \"homogeneity\"),\n    panel = FALSE \n) |> plot()\n\n\n$HOMOGENEITY\n\n\n\n\n\n\n$QQ\n\n\n\n\n\nI won’t claim to be an expert at reading model diagnostics, but these plots pretty demonstrably suggest that the model is violating assumptions of homogeneity of variance as well as normality. As suggested by our previous scatterplot, this translates to the model poorly predicting birth counts at extreme values of the predictor variable. That’s not to say a linear model isn’t useful. I think especially if you were to subset it to data within a certain order of magnitude, it would perform decently well. But let’s see if we can do better than that!"
  },
  {
    "objectID": "posts/model_live_births_from_pop_count/index.html#bayesian-modeling-using-mcmc",
    "href": "posts/model_live_births_from_pop_count/index.html#bayesian-modeling-using-mcmc",
    "title": "Modeling Live Births from Population Counts: A First-Time MCMC Model",
    "section": "Bayesian Modeling Using MCMC",
    "text": "Bayesian Modeling Using MCMC\nAs mentioned in the intro, I’m using this post as an opportunity to practice regression modeling under a Bayesian MCMC framework rather than the traditional Frequentist approach. Refer back to my last post on what makes Bayesian methods unique and when you might choose to use them. Long story short, they allow you to incorporate your prior knowledge into a model and they perform better when data is limited by small sample sizes or rare events. In this particular use case (predicting counts of live births from counts of children under 5yo), we have a large robust data set to draw upon, so I don’t expect Bayesian methods to perform that differently from Frequentist ones, but I figure that will make it easier to practice without the modeling process being too finicky.\nPackages like {brms} and {rstanarm} make it easy to take a pre-existing Frequentist model and drop it into a Bayesian framework. For example you can take our previous linear model:\n\nfreq_linear_model <- lm(births ~ pop_under_five, data = model_data)\n\nAnd wrap it in the function brm() instead:\n\nbayesian_linear_model <- brm(births ~ pop_under_five, data = model_data)\n\nIf you’re following along at home, try not to be alarmed by long output produced by the code above. Basically, the software back-end is just sending you regular updates as it approximates the model solution.\nTo find a better fit for the data than that from linear regression, let’s try modeling the outcome as a negative binomial distribution. I expect this to be more accurate because it is designed to describe discrete, count variables and it is flexible enough to fit the data even when it is very right skewed. For an improved fit, I will also log transform the predictor variable to make it more closely approximate a normal distribution:\n\nnegbinomial_model <- brm(\n    births ~ log(pop_under_five), \n    data = model_data, \n    family = negbinomial()\n)\n\nLet’s compute some metrics comparing relative performance between the two models:\n\n\nCode\ncompare_performance(bayesian_linear_model, negbinomial_model, rank = TRUE) |> print_html()\n\n\n\n\n\n\n  \n    \n      Comparison of Model Performance Indices\n    \n    \n  \n  \n    \n      Name\n      Model\n      R2\n      RMSE\n      WAIC weights\n      LOOIC weights\n      Performance-Score\n    \n  \n  \n    bayesian_linear_model\nbrmsfit\n0.99\n2479.34\n0.00e+00\n0.007\n50.00%\n    negbinomial_model\nbrmsfit\n0.98\n2650.21\n1.00\n0.993\n50.00%\n  \n  \n    \n      NA\n    \n  \n  \n\n\n\n\nThe compare_performance() function’s ranking algortihm is a little confused because the linear model does better with the data at hand; it accounts for variance in the outcome slightly better (higher R2 metric) and has slightly smaller residuals from its predictions (smaller RMSE metric). However, its important to note that both R2 and RMSE are in similar orders of magnitude for both models. More importantly for us, the WAIC and LOOIC metrics strongly favor the negative binomial model. LOOIC and WAIC try to balance between accurately fitting the data at hand while maintaining generalizability to new data. Before we generalize though, let’s visualize what the negative binomial model’s fit looks like with the data at hand:\n\n\nCode\npp_check(negbinomial_model) +\n    scale_x_log10() + \n    scale_color_manual(\n        values = c(\"red\", \"blue\"),\n        labels = c(\"Observed\", \"Predicted\")\n    ) +\n    labs(\n        title = \"Predictions Vs. Observations for Negative Binomial Model \\n Predicting Counts of Live Births from Counts of Children Under 5yo\",\n        x = \"Live Birth Count\",\n        y = \"Density\"\n    ) \n\n\n\n\n\nThe predicted blue distributions closely trace the observed red distribution, visually supporting the strong fit suggested by our metrics."
  },
  {
    "objectID": "posts/model_live_births_from_pop_count/index.html#new-predictions",
    "href": "posts/model_live_births_from_pop_count/index.html#new-predictions",
    "title": "Modeling Live Births from Population Counts: A First-Time MCMC Model",
    "section": "New Predictions",
    "text": "New Predictions\nThe differences in generalizability between the two models will become more apparent when we turn to the original goal and predict counts of live births at the census tract level. Remember that the U.S. Census provides estimates of the counts of children under 5yo in census tracts, but does not provide estimates of counts of live births. Here is the distribution of our predictor variable at the new geographic level:\n\n\nCode\ntract_data |> \n    ggplot(aes(x = pop_under_five)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Children Under 5yo in \\n Each Census Tract of Cook County Illinois from 2015-2019\",\n        x = \"Count of Children under 5yo\",\n        y = \"Density\"\n    )\n\n\n\n\n\nNotice that the counts of children under 5yo at the tract level are distributed at much lower values than those at the county level (makes sense). This should make us cautious since we’ll be extrapolating our models to a new range of data. Let’s start with what the linear model predicts:\n\n\nCode\ntract_data$lm_predicted_births <- get_predicted(bayesian_linear_model, newdata = tract_data)\n\ntract_data |> \n    ggplot(aes(x = lm_predicted_births)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Live Births Predicted Using Linear Regression \\n in Each Census Tract of Cook County, Illinois from 2015-2019\",\n        x = \"Linear Predicted Count of Live Births\",\n        y = \"Density\"\n    )\n\n\n\n\n\nUh oh, every single count is predicted by the linear model to be negative. You can’t have a negative count!\nLet’s see how the negative binomial model does:\n\n\nCode\ntract_data$nbinom_predicted_births <- get_predicted(negbinomial_model, newdata = tract_data)\n\ntract_data |> \n    ggplot(aes(x = nbinom_predicted_births)) +\n    stat_dotsinterval() +\n    labs(\n        title = \"Counts of Live Births Predicted Using Negative Binomial Regression \\n in Each Census Tract of Cook County, Illinois from 2015-2019\",\n        x = \"Negative Binomial Predicted Count of Live Births\",\n        y = \"Density\"\n    )\n\n\n\n\n\nThat makes much more sense, the predicted counts go no lower than zero, and these predictions correspond to tracts where the counts of children under 5yo were also zero:\n\n\nCode\nbind_rows(\n    head(arrange(tract_data, pop_under_five), 3),\n    slice_sample(tract_data, n = 3),\n    slice_max(tract_data, order_by = pop_under_five, n = 3)\n)\n\n\n# A tibble: 9 × 4\n        GEOID pop_under_five lm_predicted_births nbinom_predicted_births\n        <dbl>          <dbl> <gt_prdct>          <gt_prdct>             \n1 17031980100              0 -344.9329            0.000000              \n2 17031980000              0 -344.9329            0.000000              \n3 17031670100              0 -344.9329            0.000000              \n4 17031821402             88 -339.3627            4.739437              \n5 17031808301            157 -334.9951            8.508460              \n6 17031230900            266 -328.0956           14.497711              \n7 17031803606            969 -283.5968           53.554069              \n8 17031020702            923 -286.5085           50.985029              \n9 17031842100            917 -286.8883           50.650037              \n\n\nThe negative binomial model generalizes better and creates more sensible predictions in a new range of predictor values. In future work, I’ll use these estimated counts of live births to estimate the underlying risk process for sudden unexpected infant death. Thanks for reading!"
  },
  {
    "objectID": "posts/ww_eda/index.html",
    "href": "posts/ww_eda/index.html",
    "title": "Exploratory Data Analysis of SARS-CoV-2 in Cook County Wastewater",
    "section": "",
    "text": "I am working on a project to support Cook County Department of Public Health with wastewater surveillance of SARS-CoV-2. We want to detect surges of viral copies in wastewater and see if those surges can act as early warnings for surges in hospital cases of COVID-19. In this post, I will perform some preliminary exploratory data analysis of the data."
  },
  {
    "objectID": "posts/ww_eda/index.html#dataset-and-prep",
    "href": "posts/ww_eda/index.html#dataset-and-prep",
    "title": "Exploratory Data Analysis of SARS-CoV-2 in Cook County Wastewater",
    "section": "Dataset and Prep",
    "text": "Dataset and Prep\nThe data was derived from the CDC’s National Wastewater Surveillance System. Please see this git commit for a specification of the data preparation pipeline I performed using the {targets} package.\nHere is a glimpse of the data:\n\n\nCode\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(tmap)\n\nww_data <- arrow::read_parquet(here::here(\"data\", \"ww_cook_county.parquet\")) \n\nglimpse(ww_data)\n\n\nRows: 629\nColumns: 14\n$ date                              <date> 2021-11-01, 2021-11-01, 2021-11-03,…\n$ day_num                           <dbl> 610, 610, 612, 612, 617, 624, 624, 6…\n$ sample_collect_date_time          <dttm> 2021-11-01 03:00:00, 2021-11-01 03:…\n$ wwtp_name                         <chr> \"mwrdgc calumet wrp\", \"mwrdgc o'brie…\n$ short_name                        <chr> \"calumet\", \"obrien\", \"calumet\", \"obr…\n$ display_name                      <chr> \"Calumet, South Suburbs and Chicago\"…\n$ population_served                 <dbl> 1134897, 1263110, 1134897, 1263110, …\n$ longitude                         <dbl> -87.68732, -87.88357, -87.68732, -87…\n$ latitude                          <dbl> 41.81193, 42.05788, 41.81193, 42.057…\n$ pcr_target_avg_conc               <dbl> 9910.641, 58259.228, 2040.000, 19864…\n$ pcr_target_units                  <chr> \"copies/l wastewater\", \"copies/l was…\n$ rec_eff_percent                   <dbl> 0.000, 0.015, 0.000, 6.455, 3.055, 6…\n$ flow_rate                         <dbl> 354, 333, 354, 333, 354, 333, 354, 3…\n$ M_viral_copies_per_day_per_person <dbl> 0.000000000, 0.008721118, 0.00000000…\n\n\nLaboratories report results as concentration of viral copies recovered per liter of wastewater at each sampling site. In order to enable standardized comparison of samples across different sampling sites, the CDC recommends standardizing by:\n\nEfficiency of viral recovery during the sampling process (variable rec_eff_percent). This is estimated by spiking wastewater with a known quantity of a different virus and seeing what proportion is recovered.\nFlow rate of wastewater at the sampling site (variable flow_rate in millions of gallons per day).\nNumber of people supplying waste to the sewershed (variable population_served).\n\nAfter standardizing by all these variables, the measurements units convert to million viral copies per day per person (variable M_viral_copies_per_day_per_person).\nThere are 7 wastewater treatment plants at which SARS-CoV-2 is sampled in Cook County:\n\n\nCode\nww_data_locations <-\n    ww_data |>  \n    select(display_name) |> \n    distinct() |> \n    arrange(display_name)\n\nww_data_locations\n\n\n# A tibble: 8 × 1\n  display_name                          \n  <chr>                                 \n1 Calumet, South Suburbs and Chicago    \n2 Egan, Far Northwest Suburbs           \n3 Hanover Park, Far Northwest Suburbs   \n4 Kirie, Mid Northwest Suburbs          \n5 Lemont, Far Southwest Suburbs         \n6 O'Brien, Northeast Suburbs and Chicago\n7 Stickney (1), West Suburbs and Chicago\n8 Stickney (2), West Suburbs and Chicago\n\n\nHere are the locations on the map:\n\n\nCode\nww_data_locations <- \n    ww_data |>  \n    select(display_name) |> \n    distinct() |> \n    arrange(display_name) |> \n    # CDC-provided long/lats are not accurate\n    mutate(\n        longitude = c(-87.606416, -88.037690, -88.138314, -87.936888, -87.998061, -87.717100, -87.766175, -87.766175),\n        latitude = c(41.662910, 42.019824, 41.999846, 42.021035, 41.678252, 42.020932, 41.817061, 41.817061)\n    ) |> \n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n    \ntmap_mode(\"view\")\n\nww_data_locations |> \n    select(display_name) |> \n    distinct() |> \n    tm_shape() + \n    tmap::tm_markers()"
  },
  {
    "objectID": "posts/chicago_neighborhood_size/index.html",
    "href": "posts/chicago_neighborhood_size/index.html",
    "title": "How Big are Chicago Neighborhoods?",
    "section": "",
    "text": "In certain methods of spatial cross-validation, you need to set an inclusion radius and a buffer distance for the folds into which you divide your data. I am working with census tracts, and think it would make sense to use distances based on neighborhoods to set the inclusion radius and buffer distance.\n\n\n\nImage source: A and N mortgage"
  },
  {
    "objectID": "posts/chicago_neighborhood_size/index.html#set-up",
    "href": "posts/chicago_neighborhood_size/index.html#set-up",
    "title": "How Big are Chicago Neighborhoods?",
    "section": "Set-Up",
    "text": "Set-Up\nLoad the data wrangling and mapping libraries, then import Chicago neighborhoods:\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n# Source: Chicago Data Portal - Boundaries - Neighborhoods\n# URL: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Neighborhoods/bbvz-uum9\nneighborhoods <- st_read(here::here(\"data\", \"boundaries_neighborhoods_chicago.kml\"))\n\n\nReading layer `Layer0' from data source \n  `/home/riggins/blog/data/boundaries_neighborhoods_chicago.kml' \n  using driver `LIBKML'\nSimple feature collection with 98 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "posts/chicago_neighborhood_size/index.html#calculations",
    "href": "posts/chicago_neighborhood_size/index.html#calculations",
    "title": "How Big are Chicago Neighborhoods?",
    "section": "Calculations",
    "text": "Calculations\n\nUse S2 backend of sf library to calculate areas of each neighborhood\nCalculate a rough radius of each neighborhood by treating it as if it were a circle\nCalcuate the average rough radius\n\n\n\nCode\nneighborhoods |> \n    mutate(\n        area = st_area(neighborhoods),\n        rough_radius = sqrt(area/pi)\n    ) |> \n    summarize(avg_rough_radius = mean(rough_radius))\n\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n  avg_rough_radius                       geometry\n1     1273.473 [m] MULTIPOLYGON (((-87.94001 4...\n\n\nThe average rough radius is about 1273 meters."
  },
  {
    "objectID": "posts/chicago_neighborhood_size/index.html#visualize",
    "href": "posts/chicago_neighborhood_size/index.html#visualize",
    "title": "How Big are Chicago Neighborhoods?",
    "section": "Visualize",
    "text": "Visualize\nVisualize what it looks like to add a 1300 meter buffer zone around each neighborhood:\n\n\nCode\nbuffers <- st_buffer(neighborhoods, 1300)\n\n\n\n\nCode\ntmap_mode(\"view\")\ntm_shape(buffers) + tm_polygons(col = \"pri_neigh\", alpha = 0.5, border.alpha = 0) + tm_shape(neighborhoods) + tm_polygons(alpha = 0)"
  },
  {
    "objectID": "posts/beta_dist/index.html",
    "href": "posts/beta_dist/index.html",
    "title": "Estimating Risk for Rare Events in Small Areas where the Denominator is Unknown",
    "section": "",
    "text": "This blog post is an adaptation of methods taught by David Robinson in his book Introduction to Empirical Bayes: Examples from Baseball Statistics.\nMy research group has compiled a dataset that counts the number of cases of Sudden Unexpected Infant Death (SUID) in each census tract of Cook County, IL from 2015-2019. We want an accurate way to estimate the risk of SUID in each tract using this data. SUID is defined as any case of death in a baby less than age 1 year old that does not have a known cause before a medical autopsy is performed. While the state of Illinois releases statistics about SUID for the county as a whole, we think it’s important to understand this phenomenon at a more granular geographic level. That way, we have a better understanding of geographic disparities and service agencies can more precisely target their interventions.\nThe goal of this post is to convince you that Bayesian techniques offer a more principled way to estimate a risk process when the amount of observed data has limitations by itself."
  },
  {
    "objectID": "posts/beta_dist/index.html#intro-to-the-data",
    "href": "posts/beta_dist/index.html#intro-to-the-data",
    "title": "Estimating Risk for Rare Events in Small Areas where the Denominator is Unknown",
    "section": "Intro to the Data",
    "text": "Intro to the Data\nHere’s what our dataset looks like:\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\nsuid_base_table <- \n    read_csv(here::here(\"data\", \"suid_snapshot_2022_08_31.csv\")) |>\n    # Remove rows with NA values for count of SUID cases or for population under five\n    filter(!is.na(suid_count) & !is.na(pop_under_five)) |> \n    select(fips, suid_count, pop_under_five)\n\nsuid_base_table\n\n# A tibble: 1,284 × 3\n          fips suid_count pop_under_five\n         <dbl>      <dbl>          <dbl>\n 1 17031010100          0            364\n 2 17031010201          0            592\n 3 17031010202          0            257\n 4 17031010300          0            312\n 5 17031010400          0             38\n 6 17031010501          0            277\n 7 17031010502          0            139\n 8 17031010503          0             65\n 9 17031010600          1            177\n10 17031010701          0            288\n# … with 1,274 more rows\n\n\nEach row is an individual census tract. The fips column lists each tract’s unique identifier, suid_count is the number of SUID cases that took place from 2015-2019, and pop_under_five is the population of children under age five as estimated by the U.S. Census’ American Community Survey.\nIt’s important to note that SUID is a rare event. The vast majority of census tracts did not suffer any cases of SUID during this time period. Here’s a histogram to illustrate:\n\n\n\n\n\nThe first challenge in estimating the risk is we don’t have the proper denominator. SUID incidence is typically reported as cases per 100,000 live births. However, we don’t have disaggregated counts of live births in each census tract. The best approximation I have found is pop_under_five, which I hope reasonably mirrors the number of live births over a five-year period. Here are approximated incidences for each tract using pop_under_five as denominator:\n\nsuid_incidence_table <-\n    suid_base_table |> \n    mutate(suid_incidence = round(suid_count / pop_under_five * 1E5, 2))\n\nsuid_incidence_table\n\n# A tibble: 1,284 × 4\n          fips suid_count pop_under_five suid_incidence\n         <dbl>      <dbl>          <dbl>          <dbl>\n 1 17031010100          0            364             0 \n 2 17031010201          0            592             0 \n 3 17031010202          0            257             0 \n 4 17031010300          0            312             0 \n 5 17031010400          0             38             0 \n 6 17031010501          0            277             0 \n 7 17031010502          0            139             0 \n 8 17031010503          0             65             0 \n 9 17031010600          1            177           565.\n10 17031010701          0            288             0 \n# … with 1,274 more rows\n\n\nLet’s calculate the mean of these values:\n\nmean(suid_incidence_table$suid_incidence)\n\n[1] NaN\n\n\nHmm, it’s seems like there are some “Not a number” values messing up the calculation. Let’s filter them out:\n\nmean(suid_incidence_table$suid_incidence, na.rm = TRUE)\n\n[1] Inf\n\n\nThat’s also not what we were looking for. What’s going on?\nA problem that emerges from using pop_under_five as the denominator is that five tracts are estimated to have zero children under the age of five. In such cases, R calculates the incidence as NaN (not a number) when suid_count is zero and as Inf (infinity) when suid_count is anything greater than zero:\n\nsuid_incidence_table |> \n    filter(pop_under_five == 0)\n\n# A tibble: 5 × 4\n         fips suid_count pop_under_five suid_incidence\n        <dbl>      <dbl>          <dbl>          <dbl>\n1 17031081000          0              0            NaN\n2 17031280800          0              0            NaN\n3 17031380200          0              0            NaN\n4 17031670100          1              0            Inf\n5 17031834900          1              0            Inf\n\n\nWe know that the true risk for these tracts can’t be a non-existent number or infinitely large, so we’ll do our best adjust for these circumstances:\n\nsuid_incidence_table <-\n    suid_incidence_table |> \n    mutate(\n        suid_incidence = \n            case_when(\n                # If incidence is NaN, change to 0\n                is.nan(suid_incidence) ~ 0,\n                # If it's Inf, change to 100,000\n                is.infinite(suid_incidence) ~ 1E5,\n                # Otherwise, keep as is\n                TRUE ~ suid_incidence\n            )\n    )\n\nNow let’s try and calculate the mean:\n\nmean(suid_incidence_table$suid_incidence)\n\n[1] 339.1101\n\n\nA much more sensible number, although it still seems like an overestimate compared to the overall incidence for Cook County, which is 88.3 cases per 100,000 births per Illinois Department of Health.\nLet’s take a look at the census tracts with lowest and highest incidences:\n\nsuid_incidence_table |> \n    arrange(suid_incidence) |> \n    head(10)\n\n# A tibble: 10 × 4\n          fips suid_count pop_under_five suid_incidence\n         <dbl>      <dbl>          <dbl>          <dbl>\n 1 17031010100          0            364              0\n 2 17031010201          0            592              0\n 3 17031010202          0            257              0\n 4 17031010300          0            312              0\n 5 17031010400          0             38              0\n 6 17031010501          0            277              0\n 7 17031010502          0            139              0\n 8 17031010503          0             65              0\n 9 17031010701          0            288              0\n10 17031010702          0            530              0\n\n\n\nsuid_incidence_table |> \n    arrange(desc(suid_incidence)) |> \n    head(10)\n\n# A tibble: 10 × 4\n          fips suid_count pop_under_five suid_incidence\n         <dbl>      <dbl>          <dbl>          <dbl>\n 1 17031670100          1              0        100000 \n 2 17031834900          1              0        100000 \n 3 17031530502          2             15         13333.\n 4 17031081401          3             30         10000 \n 5 17031831300          1             10         10000 \n 6 17031480300          1             13          7692.\n 7 17031834700          5             92          5435.\n 8 17031612000          2             37          5405.\n 9 17031670200          4             74          5405.\n10 17031690300          2             46          4348.\n\n\nOn both extremes, the incidence values seem intuitively implausible when used to characterize the underlying risk for SUID. On one end, we shouldn’t expect that a tract’s risk for SUID was zero just because there were no observed cases. SUID is rare, so it’s expected that many tracts will count zero cases just by luck of the draw.\nOn the other end, notice how the tracts with highest incidence of SUID also tend to have low populations under five. Let’s take an aside for a minute and think about flipping a coin and using the results to estimate whether that coin is weighted to favor a certain side. If you flip a coin three times and get heads every single time, do you think it’s actually weighted to favor heads? What about if you flip it 500 times and still get heads every single time? You should be a lot more confident after 500 coin flips that the coin is truly weighted because you’ve accumulated more evidence. Similarly, think of every live birth as a coin flip that accumulates evidence about the underlying risk of SUID. The more live births there are, the more confident we can be that the observed incidence matches the true risk for SUID. Said another way, census tracts with high incidence but low counts of pop_under_five might just be very unlucky, rather than truly at higher risk.\nGiven these limitations of approximating incidence, we are going to use a Bayesian approach to adjust estimations to incorporate our “prior” expectations of what the underlying risk process should look like."
  },
  {
    "objectID": "posts/beta_dist/index.html#step-1-set-your-prior-expectations",
    "href": "posts/beta_dist/index.html#step-1-set-your-prior-expectations",
    "title": "Estimating Risk for Rare Events in Small Areas where the Denominator is Unknown",
    "section": "Step 1: Set your prior expectations",
    "text": "Step 1: Set your prior expectations\nWe are going to use the “Beta distribution” to represent our prior expectations. The Beta distribution is not nearly as well known as others like the normal distribution (aka Bell Curve), but the Beta is particularly well suited to represent a plausible values for a risk process. Check out David Robinson’s post here for more explanation. The Beta distribution has two “parameters”. The number of observed events is represented by \\(\\alpha\\) (SUID cases) and the number of trials that don’t result in an event is represented by \\(\\beta\\) (live births that don’t result in SUID, aka survivals). Let’s try using Cook County’s overall incidence of SUID and count of live births to set our prior expectations:\n\n# Sourced from Illinois Vital Statistics\n# https://dph.illinois.gov/topics-services/life-stages-populations/infant-mortality/sids/sleep-related-death-statistics.html\noverall_incidence <- 88.3 / 1E5 # per live birth for Cook County in 2014\noverall_incidence\n\n[1] 0.000883\n\n\n\n# https://dph.illinois.gov/data-statistics/vital-statistics/birth-statistics.html\ntotal_live_births <- 139398 # for Cook County from 2015-2019\nextrapolated_cases <- overall_incidence * total_live_births\nextrapolated_cases\n\n[1] 123.0884\n\n\n\nextrapolated_survivals <- total_live_births - extrapolated_cases\nextrapolated_survivals\n\n[1] 139274.9\n\n\nA cool thing about the Beta distribution is it will adjust it’s shape to reflect the number of observations (aka evidence) we give it. I’ll try and illustrate by rescaling the parameters to reflect the relative number of live births (coin flips of evidence) in the county versus in a typical census tract.\n\nscaling_factor <- median(suid_incidence_table$pop_under_five) / total_live_births\nscaling_factor\n\n[1] 0.001531586\n\n\nLet’s simulate two Beta distributions of 1,284 census tracts. One will use parameters reflecting the amount of evidence accumulated for a whole county’s worth of live births, versus just a census tract’s worth of live births.\n\n\n\n\n\nBoth distributions have the same ratio of cases (\\(\\alpha\\)) to survivals (\\(\\beta\\)), so their mean expected SUID Risk is about the same at 88.3 cases per 100,000 live births, but the blue distribution observed a county’s worth of live birth evidence (139,398), so its range of expected values is much more compact (about 70 to 120) than a census tract’s worth of live birth evidence (estimating risk anywhere from 0 to 500).\nIn Bayesian analysis, we combine our prior expectations with observed data to get a “posterior” estimate. In this case, I want the prior expectation to weigh about the same as the observed data, so I’ll use census-tract-scaled parameters in my prior."
  },
  {
    "objectID": "posts/beta_dist/index.html#step-2-combine-the-prior-expectation-and-observed-data",
    "href": "posts/beta_dist/index.html#step-2-combine-the-prior-expectation-and-observed-data",
    "title": "Estimating Risk for Rare Events in Small Areas where the Denominator is Unknown",
    "section": "Step 2: Combine the prior expectation and observed data",
    "text": "Step 2: Combine the prior expectation and observed data\nTo get each posterior estimate of risk from the Beta distribution, we are going to perform the following calculation:\n\\[ \\frac{cases_{prior} + cases_{observed}}{cases_{prior} + cases_{observed} + survivals_{prior} + survivals_{observed}} = \\frac{0.189 + cases_{observed}}{0.189 + cases_{observed} + 213.311 + survivals_{observed}} \\]\n\nprior_alpha <- extrapolated_cases * scaling_factor\nprior_beta <- extrapolated_survivals * scaling_factor\nsuid_posterior_table <-\n    suid_incidence_table |> \n    mutate(\n        posterior_alpha = prior_alpha + suid_count,\n        posterior_beta = prior_beta + pop_under_five - suid_count\n    ) |> \n    mutate(\n        posterior_risk = posterior_alpha / (posterior_alpha + posterior_beta) * 1E5,\n        # Use the Beta Distribution Quantile function to also get 95% credible intervals\n        posterior_risk_low = qbeta(0.025, posterior_alpha, posterior_beta) * 1E5,\n        posterior_risk_high = qbeta(0.975, posterior_alpha, posterior_beta) * 1E5,\n        .before = posterior_alpha\n    )\n\nsuid_posterior_table\n\n# A tibble: 1,284 × 9\n          fips suid_co…¹ pop_u…² suid_…³ poste…⁴ poste…⁵ poste…⁶ poste…⁷ poste…⁸\n         <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 17031010100         0     364      0     32.6 3.56e-7    253.   0.189    577.\n 2 17031010201         0     592      0     23.4 2.55e-7    181.   0.189    805.\n 3 17031010202         0     257      0     40.1 4.38e-7    310.   0.189    470.\n 4 17031010300         0     312      0     35.9 3.92e-7    278.   0.189    525.\n 5 17031010400         0      38      0     75.0 8.20e-7    580.   0.189    251.\n 6 17031010501         0     277      0     38.4 4.20e-7    298.   0.189    490.\n 7 17031010502         0     139      0     53.5 5.84e-7    414.   0.189    352.\n 8 17031010503         0      65      0     67.7 7.40e-7    524.   0.189    278.\n 9 17031010600         1     177    565.   304.  1.27e+1   1041.   1.19     389.\n10 17031010701         0     288      0     37.6 4.11e-7    291.   0.189    501.\n# … with 1,274 more rows, and abbreviated variable names ¹​suid_count,\n#   ²​pop_under_five, ³​suid_incidence, ⁴​posterior_risk, ⁵​posterior_risk_low,\n#   ⁶​posterior_risk_high, ⁷​posterior_alpha, ⁸​posterior_beta\n\n\nLet’s visualize how our posterior estimates of risk relate to incidence calculations and prior expectations:\n\n\n\n\n\nOn the Y axis, we have 9 census tracts for which we’ve estimated risk. We label each tract with the observed data used to calculate approximate incidence. For example, the top row shows a census tract that observed zero cases of SUID and had a population under five of 1,011 children. Each approximate incidence is marked on the x axis as a hollow circle for reference. Each filled black circle marks our posterior estimate of SUID risk and is flanked by a 95% credible interval (the Bayesian cousin to the confidence interval). The dashed vertical line marks our prior expectation of risk as observed for the whole county.\nWhat I want you to notice is how the posterior estimate represents a tug-of-war between the prior expectation (dashed line) and the observed data (hollow circle). The prior expectation is able to pull our estimates away from extreme observed incidence values (like 0 in the top three rows, or over 5000 in the bottom three rows) to more plausible values. This balance between the forces of expectation and observation is what makes Bayesian estimation so powerful!"
  },
  {
    "objectID": "posts/beta_dist/index.html#geographic-pattern",
    "href": "posts/beta_dist/index.html#geographic-pattern",
    "title": "Estimating Risk for Rare Events in Small Areas where the Denominator is Unknown",
    "section": "Geographic Pattern?",
    "text": "Geographic Pattern?\nWhen we map census tracts with the 100 highest and 100 lowest estimates of SUID risk, we notice a pattern starting to emerge. The lowest estimates are concentrating on the West/South sides of Chicago and the Southern suburbs. These areas are where historic trends of segregation have caused concentration of socioeconomic vulnerability. When you think about it, we might expect these areas to have higher risk of SUID even before we observe the data. In a future blog post, I’ll show how we can use auxiliary information about tracts (like location or SES) to fine tune our prior expectations of risk for a better posterior estimate."
  },
  {
    "objectID": "posts/suid_joint_model_design/index.html",
    "href": "posts/suid_joint_model_design/index.html",
    "title": "Designing a Joint Model of SUID Risk and Live Birth Counts",
    "section": "",
    "text": "The purpose of this post is to solicit help with building a more useful Bayesian model of SUID Risk.\n\nThe ultimate parameter of interest is the risk of sudden infant death syndrome (SUID) in each census tract of Cook County, IL from 2015-2019. I have counts of SUID cases that took place in each tract during that time period. I want to use those counts to estimate the underlying generative risk process that produced them.\nOne could naively represent the risk as raw incidence. In this blog post, I outlined why that’s a bad idea because it produces implausibly low and high point estimates. This is in part because SUID is (blessedly) rare and the population of each census tract is relatively small. For example, the overall incidence of SUID in Cook County as a whole was about 88.3 cases per 100,000 births, but incidence calculations in individual census tracts were as small as 0 cases per 100,000 births and as high as 13,000 cases per 100,000 births.\nBayesian estimation of the generative risk process brings two primary benefits:\n\nIt allows me to incorporate prior knowledge to smooth out estimates toward more plausible values\nIt allows me to represent risk as a distribution of plausible values rather than just getting a point estimate"
  },
  {
    "objectID": "posts/suid_joint_model_design/index.html#basic-premise",
    "href": "posts/suid_joint_model_design/index.html#basic-premise",
    "title": "Designing a Joint Model of SUID Risk and Live Birth Counts",
    "section": "Basic Premise",
    "text": "Basic Premise\nI want to represent the risk in the following Beta-Binomial model:\n\\[Y|\\pi \\sim Binomial(n, \\pi)\\]\n\\[\\pi \\sim Beta(\\alpha, \\beta)\\]\nWhere:\n\n\\(Y|\\pi\\) is the count of SUID cases modeled by a Binomial distribution with parameters \\(n\\) and \\(\\pi\\)\n\\(n\\) is the count of births\n\\(\\pi\\) is the risk of SUID modeled by a Beta distribution with hyperparameters \\(\\alpha\\) and \\(\\beta\\)"
  },
  {
    "objectID": "posts/suid_joint_model_design/index.html#preliminary-solution",
    "href": "posts/suid_joint_model_design/index.html#preliminary-solution",
    "title": "Designing a Joint Model of SUID Risk and Live Birth Counts",
    "section": "Preliminary Solution",
    "text": "Preliminary Solution\nAs a starting pointing, I decided to use global priors derived from incidence of SUID in Cook County as a whole in 2014:\n\\[\\mu_0 = \\sigma_0 = 88.3 \\times 10^{-5}\\]\nWhere:\n\n\\(\\mu_0\\) is the prior mean SUID risk\n\\(\\sigma_0\\) is the prior standard deviation of SUID risk\n\nThese were reparameterized to:\n\\[\\alpha_0 = \\frac{\\mu_0}{\\sigma_0} = 1\\]\n\\[\\beta_0 = \\frac{1-\\mu_0}{\\sigma_0} \\approx 1132\\]\nFrom these priors, I estimated a posterior point estimate for each tract as:\n\\[E(\\pi_i | Y = y_i) = \\frac{\\alpha_0 + y_i}{\\alpha_0 + \\beta_0 + n_i}\\]\nWhere \\(E(\\pi_i | Y = y_i)\\) is read as the posterior mean risk of SUID in an indexed census tract given \\(y_i\\) SUID cases were observed out of \\(n_i\\) births.\nSee my previous blog post for an implementation of this preliminary model."
  },
  {
    "objectID": "posts/suid_joint_model_design/index.html#limitations-and-proposed-improvements",
    "href": "posts/suid_joint_model_design/index.html#limitations-and-proposed-improvements",
    "title": "Designing a Joint Model of SUID Risk and Live Birth Counts",
    "section": "Limitations and Proposed Improvements:",
    "text": "Limitations and Proposed Improvements:\n\n1. Number of births was fudged\nWe don’t actually have data on the number of births that took place in each census tract. The U.S. Census only produces estimates of birth counts to a level of granularity at the county level. Instead, I used the Census’ data on the number of young children (less than age 5) in each census tract as a proxy for the number of births.\nIn this blog post, I outlined a generalized linear regression model of birth counts from population counts of young children at the county level using the Negative Binomial distribution. I want to incorporate this work into a joint model that estimates birth counts and SUID risk concurrently at the census tract level.\n\n\n2. Our prior knowledge doesn’t actually expect risk to be the same in each tract\nBased on extraneous socioeconomic factors, our prior knowledge expects some tracts to have greater risk of SUID than others.\nI want to create individual priors indexed to each census tract.\n\n\n3. The model ignores spatial influence\nThe current model considers risk as an independent process in each census tract. A more plausible model would modulate the estimate of risk in each tract to be more similar to estimates of risk in nearby tracts.\nI want to add such a component of spatial autocorrelation to the estimates."
  },
  {
    "objectID": "posts/suid_joint_model_design/index.html#help-wanted",
    "href": "posts/suid_joint_model_design/index.html#help-wanted",
    "title": "Designing a Joint Model of SUID Risk and Live Birth Counts",
    "section": "Help Wanted",
    "text": "Help Wanted\nI know that adding all these moving parts will require me to transition from using a mathematically-specified posterior solution, to using an MCMC approximation. I am hoping someone can help me write out all these components into a cohesive whole that can be fed into brms (or directly into Stan). I aim to get this work published and would at minimum add you into the acknowledgements section if not consider adding you as a co-author depending on the level of assistance given.\nThank you for the consideration!"
  },
  {
    "objectID": "posts/chicago_cold_deaths/index.html",
    "href": "posts/chicago_cold_deaths/index.html",
    "title": "PH425 Final Project",
    "section": "",
    "text": "Cook County Health and Housing Forward’s Recuperation in Supportive Environment (RISE) Center cares for people experiencing homelessness as they become well enough to discharge from a hospital, but still need a stable home environment in which to medically recover. RISE provides both basic medical services from 24-hr medical assistants and social services from case managers. In winter months, RISE Center experiences a strong uptick in clients admitted due to frostbite and other cold-related injuries. Often these clients have newly amputated digits or limbs, which robs them of independence with their activities of daily living. These injuries are consequences of societal policies that do not adequately prioritize protection and care for people experiencing homelessness.\nOne means of mitigating the threat of cold weather is to provide warming centers. The City of Chicago designates facilities like libraries, community centers, senior centers, and police stations as available when temperatures dip lower than 32 degrees Fahrenheit “for all residents in need of safe refuge and relief from extreme cold weather.” The objective of this analysis was to compare the locations of hypothermic deaths investigated by the Cook County Medical Examiner (CCME) to locations of warming centers. Are there geographic gaps where a significant number of deaths have taken place, but there are no warming facilities nearby?"
  },
  {
    "objectID": "posts/chicago_cold_deaths/index.html#limitations",
    "href": "posts/chicago_cold_deaths/index.html#limitations",
    "title": "PH425 Final Project",
    "section": "Limitations",
    "text": "Limitations\nThere are limitations to this preliminary analysis. First, proximity to a warming center has not been causally established to mitigate risk of cold deaths. One would need to perform a regression analysis while controlling for confounding/colliding variables to determine whether such a mitigating relationship indeed existed. Furthermore, it is possible that physical proximity to a warming center in Euclidean space may be less informative of a measure than “travel time” to a warming center by some means of accessible transportation like public bus or train. Although there are services available to calculate travel time, these services cost money, especially when trying to calculate isochronic surfaces across the whole city to multiple points."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hall Riggins Blog",
    "section": "",
    "text": "PH425 Final Project\n\n\n\n\n\n\n\ncold\n\n\nepidemiology\n\n\nchicago\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nDaniel P. Hall Riggins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis of SARS-CoV-2 in Cook County Wastewater\n\n\n\n\n\n\n\nchicago\n\n\nepidemiology\n\n\ncovid\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n  \n\n\n\n\nHow Big Are Chicago Neighborhoods?\n\n\n\n\n\n\n\ngeography\n\n\nchicago\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n  \n\n\n\n\nDesigning a Joint Model of SUID Risk and Live Birth Counts\n\n\n\n\n\n\n\nsuid\n\n\ninfant-safety\n\n\npediatrics\n\n\nepidemiology\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n  \n\n\n\n\nCharacterizing Age of Victims of Gun-Related Deaths in Cook County, IL, 2017-2021\n\n\n\n\n\n\n\nguns\n\n\nepidemiology\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n  \n\n\n\n\nModeling Live Births from Population Counts: A First-Time MCMC Model\n\n\n\n\n\n\n\nsuid\n\n\ninfant-safety\n\n\npediatrics\n\n\nepidemiology\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Makes Bayesian Methods Different and Why Should I Care?\n\n\n\n\n\n\n\nsuid\n\n\ninfant-safety\n\n\npediatrics\n\n\nepidemiology\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Risk for Rare Events in Small Areas where the Denominator is Unknown\n\n\n\n\n\n\n\nsuid\n\n\ninfant-safety\n\n\npediatrics\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2022\n\n\nDaniel P. Hall Riggins, MD\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Hall Riggins Blog",
    "section": "",
    "text": "CC0 1.0 Universal\nStatement of Purpose\nThe laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an “owner”) of an original work of authorship and/or a database (each, a “Work”).\nCertain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (“Commons”) that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.\nFor these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the “Affirmer”), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (“Copyright and Related Rights”). Copyright and Related Rights include, but are not limited to, the following:\n\n\nthe right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;\nmoral rights retained by the original author(s) and/or performer(s);\npublicity and privacy rights pertaining to a person’s image or likeness depicted in a Work;\nrights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;\nrights protecting the extraction, dissemination, use and reuse of data in a Work;\ndatabase rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and\nother similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.\n\n\nWaiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer’s Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “Waiver”). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer’s heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer’s express Statement of Purpose.\nPublic License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer’s express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer’s Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “License”). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer’s express Statement of Purpose.\nLimitations and Disclaimers.\n\n\nNo trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.\nAffirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.\nAffirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person’s Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.\nAffirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work.\n\nFor more information, please see http://creativecommons.org/publicdomain/zero/1.0/"
  }
]